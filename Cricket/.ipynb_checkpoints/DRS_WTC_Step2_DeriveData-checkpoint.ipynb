{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A study to understand how teams, players and umpires have used referrals in the World Test Championship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible insights desired\n",
    "\n",
    "**PHASE 1**:\n",
    "\n",
    "- How effective was the DRS call in extending the survival at the crease?\n",
    "\n",
    "- No. of referrals innings wise\n",
    "\n",
    "- Who was the batting partner who has probably assited the most in DRS?\n",
    "\n",
    "**PHASE 2**:\n",
    "\n",
    "- How many recognized batsmen were left? And was there a missed opportunity due to DRS being recklessly taken earlier?**\n",
    "\n",
    "- Missed reviews by teams: Did # of remaining reviews have a say\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data points collected (Phase 1)\n",
    "-  Match in Series \n",
    "-  Series Name to produce facets \n",
    "-  Match Venue\n",
    "-  Match Date (Month_Year)\n",
    "-  Over of referral\n",
    "-  Innings of referral in game\n",
    "- Team taking review\n",
    "- Team Batting/Bowling\n",
    "- Umpire at time of review\n",
    "- Batsman at time of review\n",
    "- Outcome of review \n",
    "- Innings wise dismissal data\n",
    "- Active wicket partnership\n",
    "- Innings wise referral data (scraped from match notes, needs a bit of formatting)\n",
    "- Commentary of that particular referral ball\n",
    "\n",
    "\n",
    "### Data points (Phase 2)\n",
    "\n",
    "- no. of recognized batsman to come vs missed opportunities for them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all match wise referral DF into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k.shridhar\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "reviews_df=pd.DataFrame()\n",
    "\n",
    "for glob in glob.glob(\"review_data/*.csv\"):\n",
    "    df=pd.read_csv(glob)\n",
    "    reviews_df=pd.concat([df,reviews_df])\n",
    "    \n",
    "# reviews_df.to_csv('reviews.csv',index=False)\n",
    "\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_df=pd.read_csv('review_data/reviews.csv')\n",
    "reviews_df['innings_action']=(reviews_df['Review_team'].astype('str')).apply(lambda x: \"Batting\" if \"Batting\" in x else \"Bowling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process review events- to add overturned, review loss, 1st review loss, 2nd review loss logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=reviews_df.loc[(~pd.isnull(reviews_df.reviews))]\n",
    "\n",
    "# df=df[df.match=='eng-vs-aus-3rd-test-the-ashes-2019']\n",
    "# ##Split into innings\n",
    "# reviews_df['Review_team']=reviews_df['Review_team'].astype('str')\n",
    "# df=reviews_df\n",
    "# df.fillna('',inplace=True)\n",
    "##Keep start review count =2\n",
    "## Overturn event logic. bowling-upheld and p.broken=True [OR] batting-upheld with p.broken=False \n",
    "##1st review lost at =\n",
    "##2nd review lost at = \n",
    "## Review loss logic is take innings, take team batting- check upheld or struck down or struck down umpires call. add 1 if struck down\n",
    "## for team bowling- check upheld or struck down or stuck down umpires call. if struck down, then add 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overturned']=df[['Review_team','Review_outcome','Partnership_broken']].apply(lambda x: True if \n",
    "(('Bowling' in x.Review_team and x.Review_outcome=='Upheld' and x.Partnership_broken==True) \n",
    " or ('Batting' in x.Review_team and x.Review_outcome=='Upheld' and x.Partnership_broken==False)) else False,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_lost']=df[['Review_team','Review_outcome','Partnership_broken']].apply(lambda x: True if \n",
    "(('Bowling' in x.Review_team and x.Review_outcome=='Struck down' and x.Partnership_broken==False) \n",
    " or ('Batting' in x.Review_team and x.Review_outcome=='Struck down' and x.Partnership_broken==True)) else False,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_review_lost']=(df.loc[df.review_lost==True].sort_values('Over').groupby([\"match\",\"innings\",\"innings_action\"])['review_lost'].rank('first')==1)\n",
    "\n",
    "df['second_review_lost']=(df.loc[df.review_lost==True].sort_values('Over').groupby([\"match\",\"innings\",\"innings_action\"])['review_lost'].rank('first')==2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_add_from_df=['overturned','review_lost','first_review_lost','second_review_lost']\n",
    "\n",
    "for col in cols_to_add_from_df:\n",
    "    reviews_df[col]=df.loc[df.index,col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisations for blog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batsman who took the most reviews (intiated by batsman)\n",
    "- Whom did teams take a review against the most? \n",
    "- Umpire effectiveness (# of overturned decisions/total number of DRS calls for an umpire) Bar chart\n",
    "- Review effectiveness by teams overall (Who had most partnership breaks when overturned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=reviews_df[~pd.isnull(reviews_df.reviews)]\n",
    "\n",
    "df.shape\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overturned decisions per match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overturned_per_match=df.groupby('match')['overturned'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total decisions per match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_per_match=df.groupby('match')['overturned'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_decisions=[]\n",
    "\n",
    "tot_avg_decisions=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range((df.shape[0])):\n",
    "    ot_decisions.append(overturned_per_match.loc[df.match.iloc[r]])\n",
    "    tot_avg_decisions.append(overturned_per_match.loc[df.match.iloc[r]]/total_per_match.loc[df.match.iloc[r]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overturned_decisions']=ot_decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ratio_overturned']=tot_avg_decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.bar(df,x='match',y='overturned_decisions')\n",
    "# # fig.update_xaxes(categoryorder='sum descending')\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['team']=df['Review_team'].apply(lambda x:x.split('(')[0].strip())\n",
    "\n",
    "df['bowler']=df['Commentary'].apply(lambda x:x.split('to')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
